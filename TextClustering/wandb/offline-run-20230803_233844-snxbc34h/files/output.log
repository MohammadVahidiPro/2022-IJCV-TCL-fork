Network(
  (backbone): SentenceTransformer(
    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel
    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})
    (2): Normalize()
  )
  (instance_projector): Sequential(
    (0): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=384, out_features=384, bias=True)
    (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): ReLU()
    (5): Linear(in_features=384, out_features=128, bias=True)
  )
  (cluster_projector): Sequential(
    (0): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (1): ReLU()
    (2): Linear(in_features=384, out_features=384, bias=True)
    (3): BatchNorm1d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    (4): ReLU()
    (5): Linear(in_features=384, out_features=20, bias=True)
  )
)
generated augmented sentences with EDA for ./datasets/Biomedical.txt to ./datasets/BiomedicalEDA_aug.txt with num_aug=1
roberta aug: the iter 0 / 10.0
roberta aug: the iter 1 / 10.0
roberta aug: the iter 2 / 10.0
roberta aug: the iter 3 / 10.0
roberta aug: the iter 4 / 10.0
roberta aug: the iter 5 / 10.0
roberta aug: the iter 6 / 10.0
roberta aug: the iter 7 / 10.0
roberta aug: the iter 8 / 10.0
roberta aug: the iter 9 / 10.0
$$$ timer $$$ <perform_augmentation> took 2.7e+01 mins
Step [0/78]	 loss_instance: 5.517571449279785	 loss_cluster: 3.665546178817749
Step [50/78]	 loss_instance: 4.827712059020996	 loss_cluster: 3.0050697326660156
$$$ timer $$$ <..........train> took 4.7 mins
Epoch [1/3]	 Loss: 8.02329438771957
generated augmented sentences with EDA for ./datasets/Biomedical.txt to ./datasets/BiomedicalEDA_aug.txt with num_aug=1
roberta aug: the iter 0 / 10.0
roberta aug: the iter 1 / 10.0
roberta aug: the iter 2 / 10.0
roberta aug: the iter 3 / 10.0
roberta aug: the iter 4 / 10.0
roberta aug: the iter 5 / 10.0
roberta aug: the iter 6 / 10.0
roberta aug: the iter 7 / 10.0
roberta aug: the iter 8 / 10.0
roberta aug: the iter 9 / 10.0
$$$ timer $$$ <perform_augmentation> took 2.7e+01 mins
Step [0/78]	 loss_instance: 4.8235626220703125	 loss_cluster: 2.9487812519073486
Step [50/78]	 loss_instance: 4.7662811279296875	 loss_cluster: 2.941148042678833
$$$ timer $$$ <..........train> took 4.8 mins
Epoch [2/3]	 Loss: 7.7103666709019585
generated augmented sentences with EDA for ./datasets/Biomedical.txt to ./datasets/BiomedicalEDA_aug.txt with num_aug=1
roberta aug: the iter 0 / 10.0
roberta aug: the iter 1 / 10.0
roberta aug: the iter 2 / 10.0
roberta aug: the iter 3 / 10.0
roberta aug: the iter 4 / 10.0
roberta aug: the iter 5 / 10.0
roberta aug: the iter 6 / 10.0
roberta aug: the iter 7 / 10.0
roberta aug: the iter 8 / 10.0
roberta aug: the iter 9 / 10.0
$$$ timer $$$ <perform_augmentation> took 2.7e+01 mins
Step [0/78]	 loss_instance: 4.751082420349121	 loss_cluster: 2.9251673221588135
Step [50/78]	 loss_instance: 4.688261985778809	 loss_cluster: 2.9102437496185303
$$$ timer $$$ <..........train> took 4.9 mins
Epoch [3/3]	 Loss: 7.668407745850392
C:\Users\98914\anaconda3\envs\tcl-text\Lib\site-packages\torch\utils\data\dataloader.py:560: UserWarning: This DataLoader will create 10 worker processes in total. Our suggested max number of worker in current system is 8 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(_create_warning_msg(